{"cells":[{"cell_type":"markdown","id":"691954a2-3586-4a67-aad5-815d632b2654","metadata":{"language":"python"},"source":"# Documentation website scraper \n\n1. Specify the source Sitemap for documentation\n2. Specify the target desitnation\n3. Retrieve the Lost modified value along with all valid urls to be scraped\n4. Scrape the content from the URL only if the the corresponding record doesn't exist or if the last mod value of url is greater than the lastmod value in storage ( indicating that docs have been updated for that URL)\n5. Run scraping for multiple pages in parallel\n6. Store the html content and also the translated markdown content in the target table\n7. Exist when no new urls are left"},{"cell_type":"raw","id":"512519ba-4db2-49c8-abf3-4c4e5a02f9a8","metadata":{"execution":{"iopub.execute_input":"2025-02-26T18:05:24.764088Z","iopub.status.busy":"2025-02-26T18:05:24.763446Z","iopub.status.idle":"2025-02-26T18:05:34.014883Z","shell.execute_reply":"2025-02-26T18:05:34.002575Z","shell.execute_reply.started":"2025-02-26T18:05:24.764050Z"},"language":"python"},"source":"!pip install bs4 asyncio nest_asyncio aiohttp html2text"},{"cell_type":"code","execution_count":null,"id":"264197a7-5310-48eb-aab4-6d18681c0426","metadata":{"language":"python","trusted":true},"outputs":[],"source":"import os\nimport time\nimport datetime\nimport requests\nimport xml.etree.ElementTree as ET\nfrom bs4 import BeautifulSoup\nimport asyncio\nimport aiohttp\nimport nest_asyncio\nimport traceback\nimport html2text\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nimport singlestoredb as s2\nfrom singlestoredb.management import get_secret"},{"cell_type":"code","execution_count":null,"id":"3947eb1f-598c-4416-a439-5eed9aba0a32","metadata":{"language":"python","trusted":true},"outputs":[],"source":"# Retrieve credentials from environment / secrets\nTARGET_HOST = get_secret('host')\nTARGET_USER = get_secret('user')\nTARGET_PASSWORD = get_secret('ad_pwd')\nTARGET_DATABASE = \"knowlagent\"\nTABLE_NAME = \"s2docs\"\nCONCURRENCY = 10\nSPIDER_SITEMAP_URL = \"https://docs.singlestore.com/sitemap-0.xml\""},{"cell_type":"code","execution_count":null,"id":"1ed3eb91-0ea3-4155-a617-0bd24462bb5d","metadata":{"language":"python","trusted":true},"outputs":[],"source":"def execute_query(query, params=None, commit=False):\n    \"\"\"\n    Execute a query on the SingleStore database. Closes connection after.\n    \"\"\"\n    connection = s2.connect(\n        host=TARGET_HOST,\n        port='3306',\n        user=TARGET_USER,\n        password=TARGET_PASSWORD,\n        database=TARGET_DATABASE\n    )\n    result = None\n    try:\n        with connection.cursor() as cur:\n            if params:\n                cur.execute(query, params)\n            else:\n                cur.execute(query)\n            if commit:\n                connection.commit()\n            else:\n                result = cur.fetchall()\n    except Exception as e:\n        tb = traceback.format_exc()\n        print(f\"Error executing query: {e}\\n{tb}\")\n    finally:\n        connection.close()\n    return result\n\n\ndef create_table():\n    \"\"\"\n    Create the documents table if it does not exist.\n    The table stores:\n      - id: primary key\n      - url: scraped URL (as TEXT, to allow very long URLs)\n      - content: full HTML content\n      - md_content:  parsed content in markdown\n      - scraped_at: UTC timestamp\n      - lastmod: when the content was last modified on the source site\n    \"\"\"\n    create_table_query = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        url TEXT,\n        content LONGTEXT,\n        md_content LONGTEXT NULL,\n        scraped_at DATETIME,\n        lastmod DATETIME\n    );\n    \"\"\"\n    execute_query(create_table_query, commit=True)\n    print(f\"Table '{TABLE_NAME}' is ready.\")\n\n\ndef get_sitemap_urls(sitemap_url):\n    \"\"\"\n    Retrieve and parse the sitemap XML to extract all URLs listed with their lastmod dates.\n    Returns a list of tuples (url, lastmod) where lastmod is a datetime object or None.\n    \"\"\"\n    try:\n        response = requests.get(sitemap_url, timeout=10)\n        response.raise_for_status()\n        root = ET.fromstring(response.content)\n        url_data = []\n        \n        # Try without namespace first\n        for url_elem in root.findall('.//url'):\n            loc = url_elem.find('loc')\n            lastmod_elem = url_elem.find('lastmod')\n            \n            if loc is not None and loc.text:\n                url = loc.text.strip()\n                lastmod = None\n                \n                # Parse lastmod if available\n                if lastmod_elem is not None and lastmod_elem.text:\n                    try:\n                        # Try to parse the date format in the sitemap\n                        lastmod_str = lastmod_elem.text.strip()\n                        # Handle common date formats\n                        if 'T' in lastmod_str:\n                            # ISO format with timezone\n                            lastmod = datetime.datetime.fromisoformat(lastmod_str.replace('Z', '+00:00'))\n                        else:\n                            # Simple YYYY-MM-DD format\n                            lastmod = datetime.datetime.strptime(lastmod_str, '%Y-%m-%d')\n                    except ValueError:\n                        print(f\"Could not parse lastmod date for {url}: {lastmod_elem.text}\")\n                \n                url_data.append((url, lastmod))\n                \n        # If we didn't find any URLs, try with namespace\n        if not url_data:\n            namespace = {'ns': root.tag.split('}')[0].strip('{')}\n            for url_elem in root.findall('ns:url', namespace):\n                loc = url_elem.find('ns:loc', namespace)\n                lastmod_elem = url_elem.find('ns:lastmod', namespace)\n                \n                if loc is not None and loc.text:\n                    url = loc.text.strip()\n                    lastmod = None\n                    \n                    # Parse lastmod if available\n                    if lastmod_elem is not None and lastmod_elem.text:\n                        try:\n                            lastmod_str = lastmod_elem.text.strip()\n                            if 'T' in lastmod_str:\n                                lastmod = datetime.datetime.fromisoformat(lastmod_str.replace('Z', '+00:00'))\n                            else:\n                                lastmod = datetime.datetime.strptime(lastmod_str, '%Y-%m-%d')\n                        except ValueError:\n                            print(f\"Could not parse lastmod date for {url}: {lastmod_elem.text}\")\n                    \n                    url_data.append((url, lastmod))\n                    \n        return url_data\n    except Exception as e:\n        tb = traceback.format_exc()\n        print(f\"Error fetching or parsing sitemap: {e}\\n{tb}\")\n        return []\n\n\ndef get_scraped_urls_with_lastmod():\n    \"\"\"\n    Batch query: get all URLs that have already been scraped with their lastmod date.\n    Returns a dictionary mapping URL to lastmod datetime.\n    \"\"\"\n    query = f\"SELECT url, lastmod FROM {TABLE_NAME};\"\n    rows = execute_query(query)\n    return {row[0]: row[1] for row in rows} if rows else {}\n\n\ndef convert_html_to_markdown(html_content):\n    \"\"\"\n    Convert HTML content to markdown format.\n    \n    Args:\n        html_content (str): HTML content to convert\n        \n    Returns:\n        str: Markdown formatted content\n    \"\"\"\n    try:\n        # Configure html2text\n        h = html2text.HTML2Text()\n        h.ignore_links = False\n        h.ignore_images = False\n        h.ignore_tables = False\n        h.body_width = 0  # No wrapping\n        h.unicode_snob = True  # Use Unicode instead of ASCII\n        h.single_line_break = True  # Use single line breaks\n        \n        # Convert HTML to markdown\n        markdown_content = h.handle(html_content)\n        return markdown_content\n    except Exception as e:\n        tb = traceback.format_exc()\n        print(f\"Error converting HTML to markdown: {e}\\n{tb}\")\n        return None\n\n\ndef insert_page(page_url, content, lastmod=None):\n    \"\"\"\n    Insert the scraped page into the database with lastmod information and markdown content.\n    \"\"\"\n    now = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n    \n    # If lastmod is None, use the current time\n    if lastmod is None:\n        lastmod = now\n    elif isinstance(lastmod, datetime.datetime):\n        lastmod = lastmod.strftime('%Y-%m-%d %H:%M:%S')\n    \n    # Convert HTML content to markdown\n    md_content = convert_html_to_markdown(content)\n    \n    # Insert with markdown content\n    query = f\"INSERT INTO {TABLE_NAME} (url, content, scraped_at, lastmod, md_content) VALUES (%s, %s, %s, %s, %s)\"\n    try:\n        execute_query(query, (page_url, content, now, lastmod, md_content), commit=True)\n    except Exception as e:\n        tb = traceback.format_exc()\n        print(f\"Error inserting {page_url}: {e}\\n{tb}\")\n\n\nasync def async_scrape_page(session, page_url, retries=3):\n    \"\"\"\n    Asynchronously scrape page content using aiohttp with a retry mechanism in case of timeouts/errors.\n    \"\"\"\n    for attempt in range(1, retries+1):\n        try:\n            # Increase timeout from 10 to 30 seconds\n            async with session.get(page_url, timeout=30) as response:\n                response.raise_for_status()\n                text = await response.text()\n                soup = BeautifulSoup(text, 'html.parser')\n                return page_url, soup.prettify()\n        except asyncio.TimeoutError as te:\n            tb = traceback.format_exc()\n            print(f\"TimeoutError scraping {page_url} on attempt {attempt}: {te}\\n{tb}\")\n        except Exception as e:\n            tb = traceback.format_exc()\n            print(f\"Error scraping {page_url} on attempt {attempt}: {e}\\n{tb}\")\n        # Wait before retrying\n        await asyncio.sleep(2)\n    # After retries, return None\n    return page_url, None\n\n\nasync def async_run_scraper(concurrency=CONCURRENCY,SPIDER_SITEMAP_URL=SPIDER_SITEMAP_URL):\n    \"\"\"\n    Main async function that:\n      1. Retrieves the sitemap URLs and batch-checks already scraped URLs.\n      2. Scrapes pending pages concurrently using aiohttp, limited by a semaphore for concurrency.\n      3. Only scrapes pages that are new or have been updated since last scraped.\n      4. Prints progress updates every 1% of pending pages processed.\n    \"\"\"\n    url_data = get_sitemap_urls(SPIDER_SITEMAP_URL)\n    total = len(url_data)\n    print(f\"Found {total} URLs in sitemap.\")\n\n    # Get existing URLs with their lastmod dates\n    scraped_data = get_scraped_urls_with_lastmod()\n    \n    # Filter URLs that need scraping (new or updated)\n    pending_urls = []\n    for url, lastmod in url_data:\n        if url not in scraped_data:\n            # New URL, needs scraping\n            pending_urls.append((url, lastmod))\n        elif lastmod is not None and scraped_data[url] is not None:\n            # URL exists, check if it's been updated\n            if lastmod > scraped_data[url]:\n                # Content has been updated, needs re-scraping\n                pending_urls.append((url, lastmod))\n    \n    pending_total = len(pending_urls)\n    print(f\"{pending_total} URLs pending scraping (after checking for updates).\")\n\n    last_percent = 0\n    sem = asyncio.Semaphore(concurrency)\n\n    async def bounded_scrape(url_info, session):\n        url, lastmod = url_info\n        async with sem:\n            return await async_scrape_page(session, url), lastmod\n\n    async with aiohttp.ClientSession() as session:\n        tasks = [bounded_scrape(url_info, session) for url_info in pending_urls]\n        for idx, future in enumerate(asyncio.as_completed(tasks), start=1):\n            (page_url, content), lastmod = await future\n            if content:\n                insert_page(page_url, content, lastmod)\n            \n            current_percent = (idx / pending_total) * 100\n            if current_percent - last_percent >= 1 or idx == pending_total:\n                print(f\"Progress: {current_percent:.1f}% ({idx}/{pending_total})\")\n                last_percent = current_percent\n\n\ndef run_scraper():\n    \"\"\"\n    Run the async scraper.\n    \"\"\"\n    try:\n        asyncio.run(async_run_scraper())\n    except RuntimeError as e:\n        # If an event loop is already running, apply nest_asyncio to allow nested loops\n        import nest_asyncio\n        nest_asyncio.apply()\n        asyncio.run(async_run_scraper())\n\n\ndef update_lastmod_from_sitemap():\n    \"\"\"\n    Update the lastmod column values in the database to match those in the sitemap.\n    Processes in batches to avoid memory issues.\n    \"\"\"\n    print(\"Updating lastmod values from sitemap...\")\n    SPIDER_SITEMAP_URL = \"https://docs.singlestore.com/sitemap-0.xml\"\n    \n    # Get all URLs and their lastmod values from the sitemap\n    url_data = get_sitemap_urls(SPIDER_SITEMAP_URL)\n    if not url_data:\n        print(\"No URLs found in sitemap or error parsing sitemap.\")\n        return\n    \n    # Get a dictionary of URLs in the database\n    query = f\"SELECT id, url FROM {TABLE_NAME};\"\n    db_rows = execute_query(query)\n    if not db_rows:\n        print(\"No URLs found in database.\")\n        return\n        \n    # Create a lookup from URL to id\n    db_url_to_id = {row[1]: row[0] for row in db_rows}\n    \n    # Update lastmod values in batches\n    batch_size = 100\n    update_count = 0\n    total_urls = len(url_data)\n    print(f\"Found {total_urls} URLs in sitemap.\")\n    \n    # Group updates in batches\n    batches = []\n    current_batch = []\n    \n    for url, lastmod in url_data:\n        if url in db_url_to_id and lastmod is not None:\n            current_batch.append((db_url_to_id[url], lastmod))\n            if len(current_batch) >= batch_size:\n                batches.append(current_batch)\n                current_batch = []\n    \n    # Add any remaining items\n    if current_batch:\n        batches.append(current_batch)\n    \n    # Process each batch\n    for batch_idx, batch in enumerate(batches, 1):\n        # Generate update query with multiple values\n        update_values = []\n        for id_val, lastmod in batch:\n            lastmod_str = lastmod.strftime('%Y-%m-%d %H:%M:%S')\n            update_values.append(f\"WHEN {id_val} THEN '{lastmod_str}'\")\n        \n        id_list = ', '.join(str(id_val) for id_val, _ in batch)\n        update_query = f\"\"\"\n        UPDATE {TABLE_NAME} \n        SET lastmod = CASE id \n            {' '.join(update_values)}\n        END\n        WHERE id IN ({id_list});\n        \"\"\"\n        \n        try:\n            execute_query(update_query, commit=True)\n            update_count += len(batch)\n            print(f\"Processed batch {batch_idx}/{len(batches)} ({update_count}/{total_urls} URLs)\")\n            # Small delay to avoid memory pressure\n            time.sleep(0.5)\n        except Exception as e:\n            tb = traceback.format_exc()\n            print(f\"Error updating batch {batch_idx}: {e}\\n{tb}\")\n    \n    print(f\"Updated lastmod values for {update_count} URLs from sitemap.\")"},{"cell_type":"code","execution_count":null,"id":"dd919465-d7a5-4836-b194-75b96e38c51b","metadata":{"language":"python","trusted":true},"outputs":[],"source":"# prepare target tables if they dont exist\ncreate_table()\n# Run scraper (e.g. via cron), using asynchronous scraping.\nrun_scraper()\nprint(\"Scraping complete.\")"}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}