{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3a187d-3628-4767-8a4f-9064094b6181",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# SQrL ChatBot with Conversations DB support\n",
    "\n",
    "1. This notebooks provides backend endpoints fetch LLM responses from InferenceAPI endpoints\n",
    "2. InferenceAPI's can be switched to any but ensure that the underlying model supports tool calling.\n",
    "3. We have added 2 tools : google_search and hybrid_search_cloud_function\n",
    "4. All queries related to singlestore will be routed to hybrid_search_cloud_function to retrieve relevant Singlestore information and then sent as context to LLM. Incase the Hybrid Search endpoint has an eeror, we fallback to Google Search results as context. All non-singlestore queries are served with google search results. No safeguards have been implemented so please query responsibly.\n",
    "5. All queries and tool results and Stitched LLM responses are stored in conversations and messages table in the database automatically.\n",
    "6. Tool calls are completely configurable. No LLM frameworks have been used, so the underlying logic can be extended to production usecase. \n",
    "7. Endpoint to fetch streaming responses are provided.\n",
    "8. Common endpoints for conversation managent are available for frontend to use. Ensure that `conversations` and `messages` tables are created and ready for use\n",
    "9. E2E tests are provided at the end to easily test the Cloud function endpoints without having to deploy them first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4b232e4-1838-444c-95eb-a72fe6b52ca1",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd94acfe-e463-4f7a-9a32-80090b40307b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:06.799133Z",
     "iopub.status.busy": "2025-03-09T02:29:06.798196Z",
     "iopub.status.idle": "2025-03-09T02:29:13.623492Z",
     "shell.execute_reply": "2025-03-09T02:29:13.533716Z",
     "shell.execute_reply.started": "2025-03-09T02:29:06.799058Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a44496b-2fca-490a-974b-163603961749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:13.766496Z",
     "iopub.status.busy": "2025-03-09T02:29:13.763189Z",
     "iopub.status.idle": "2025-03-09T02:29:15.931518Z",
     "shell.execute_reply": "2025-03-09T02:29:15.930922Z",
     "shell.execute_reply.started": "2025-03-09T02:29:13.766418Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import uuid\n",
    "\n",
    "import singlestoredb as s2\n",
    "import singlestoredb.apps as apps\n",
    "\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from requests.exceptions import HTTPError, Timeout, RequestException\n",
    "from singlestoredb.management import get_secret\n",
    "from typing import List, Dict, Optional, Any, AsyncGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a7494f-1b02-4546-b2e0-8eab96e7d7a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:15.944734Z",
     "iopub.status.busy": "2025-03-09T02:29:15.938830Z",
     "iopub.status.idle": "2025-03-09T02:29:15.965282Z",
     "shell.execute_reply": "2025-03-09T02:29:15.962954Z",
     "shell.execute_reply.started": "2025-03-09T02:29:15.942907Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "045f0b95-7dcb-4741-81f3-760f62fcda86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:15.969271Z",
     "iopub.status.busy": "2025-03-09T02:29:15.968925Z",
     "iopub.status.idle": "2025-03-09T02:29:17.149662Z",
     "shell.execute_reply": "2025-03-09T02:29:17.148098Z",
     "shell.execute_reply.started": "2025-03-09T02:29:15.969245Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fetch All environment values\n",
    "SERPAPI_API_KEY = get_secret('SERPAPI_API_KEY')\n",
    "\n",
    "# Created new on using GraphQL mutation \"expiresAt\": \"2025-04-06T00:07:35.48479758Z\"\n",
    "HYBRID_SEARCH_AUTH = get_secret('HYBRID_SEARCH_AUTH')\n",
    "# API endpoint URL\n",
    "HYBRID_SEARCH_URL = \"https://apps.aws-virginia-nb2.svc.singlestore.com:8000/functions/6fc46137-1974-4fb0-817f-dbfe782fe3f3/v2/search\"\n",
    "\n",
    "MODEL_RESPONSE_TEMPERATURE = 0\n",
    "MODEL_RESPONSE_MAX_TOKENS = 4096\n",
    "MODEL_RESPONSE_TIMEOUT = None\n",
    "MODEL_RESPONSE_RETRIES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd7bab1-d4a8-4f47-8356-4ed012b5de4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.156185Z",
     "iopub.status.busy": "2025-03-09T02:29:17.154987Z",
     "iopub.status.idle": "2025-03-09T02:29:17.166614Z",
     "shell.execute_reply": "2025-03-09T02:29:17.165734Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.156139Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This inference API is present in the S2DBPM RAG Project Org : https://portal.singlestore.com/organizations/8bd790ef-6703-4100-85c0-bb19744acc16/inference-apis/\n",
    "INFERENCE_API_ENDPOINT = \"https://apps.aws-virginia-nb2.svc.singlestore.com:8000/modelasaservice/8345ca8d-70f6-40d7-a0c1-b2e54c23babc/v1\"\n",
    "INFERENCE_MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "# Generated from GraphQL mutation Valid until 2025-04-05T19:01:42.50444973Z\n",
    "INFERENCE_AUTH_TOKEN = get_secret('INFERENCE_AUTH_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa652236-eed3-4b7b-a0b2-dd828d5b5f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.168443Z",
     "iopub.status.busy": "2025-03-09T02:29:17.167314Z",
     "iopub.status.idle": "2025-03-09T02:29:17.286204Z",
     "shell.execute_reply": "2025-03-09T02:29:17.285583Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.168415Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Database interface facing functions\n",
    "@contextmanager\n",
    "def get_db_connection():\n",
    "    \"\"\"Provides a database connection\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = s2.connect(\n",
    "            database=\"knowlagent\"\n",
    "        )\n",
    "        yield conn\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def test_db_connection():\n",
    "    try:\n",
    "        with get_db_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT 1\")\n",
    "            result = cursor.fetchone()\n",
    "            print(f\"Database connection successful: {result}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Database connection failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def init_database():\n",
    "    \"\"\"Tables have already been created separately, so this is just a placeholder\"\"\"\n",
    "    logger.info(\"Database tables were created separately, skipping initialization\")\n",
    "    return True\n",
    "\n",
    "# Database operations that mirror current in-memory operations\n",
    "def db_create_conversation(system_message):\n",
    "    \"\"\"Create a new conversation with system message\"\"\"\n",
    "    conversation_id = str(uuid.uuid4())\n",
    "    \n",
    "    try:\n",
    "        with get_db_connection() as conn:\n",
    "            # Insert the conversation\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO conversations (conversation_id) VALUES (%s)\",\n",
    "                (conversation_id,)\n",
    "            )\n",
    "            \n",
    "            # Add system message\n",
    "            message_id = str(uuid.uuid4())\n",
    "            cursor.execute(\n",
    "                \"\"\"INSERT INTO messages \n",
    "                   (message_id, conversation_id, role, content, sequence_order) \n",
    "                   VALUES (%s, %s, %s, %s, %s)\"\"\",\n",
    "                (message_id, conversation_id, \"system\", system_message, 0)\n",
    "            )\n",
    "            \n",
    "            # Commit the transaction\n",
    "            conn.commit()\n",
    "            \n",
    "        return conversation_id\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database error in db_create_conversation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def db_get_conversation(conversation_id):\n",
    "    \"\"\"Get all messages for a conversation\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        # Use dictionary cursor if available\n",
    "        try:\n",
    "            cursor = conn.cursor(dictionary=True)\n",
    "        except TypeError:\n",
    "            # If dictionary cursor is not supported\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "        cursor.execute(\n",
    "            \"\"\"SELECT role, content, tool_calls, tool_call_id \n",
    "               FROM messages \n",
    "               WHERE conversation_id = %s \n",
    "               ORDER BY sequence_order\"\"\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        \n",
    "        messages = []\n",
    "        for row in cursor.fetchall():\n",
    "            # Handle different cursor return types\n",
    "            if hasattr(row, 'keys'):  # Dictionary-like cursor\n",
    "                message = {\"role\": row[\"role\"], \"content\": row[\"content\"]}\n",
    "                tool_calls = row[\"tool_calls\"]\n",
    "                tool_call_id = row[\"tool_call_id\"]\n",
    "            else:  # Tuple-like cursor\n",
    "                message = {\"role\": row[0], \"content\": row[1]}\n",
    "                tool_calls = row[2]\n",
    "                tool_call_id = row[3]\n",
    "            \n",
    "            # Handle tool calls\n",
    "            if tool_calls and message[\"role\"] == \"assistant\":\n",
    "                try:\n",
    "                    tool_calls_data = json.loads(tool_calls)\n",
    "                    message[\"tool_calls\"] = tool_calls_data\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Handle tool responses\n",
    "            if message[\"role\"] == \"tool\" and tool_call_id:\n",
    "                message[\"tool_call_id\"] = tool_call_id\n",
    "            \n",
    "            messages.append(message)\n",
    "        \n",
    "        return messages\n",
    "\n",
    "def db_add_message(conversation_id, message):\n",
    "    \"\"\"Add a message to a conversation\"\"\"\n",
    "    # Max length for MEDIUMTEXT (adjust if using different type)\n",
    "    MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB\n",
    "    MAX_SAFE_LENGTH = 15 * 1024 * 1024     # 15MB to be safe to prevent memory issues\n",
    "    \n",
    "    with get_db_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Update last_updated timestamp\n",
    "        cursor.execute(\n",
    "            \"UPDATE conversations SET last_updated = CURRENT_TIMESTAMP WHERE conversation_id = %s\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        \n",
    "        # Get next sequence number\n",
    "        cursor.execute(\n",
    "            \"\"\"SELECT COALESCE(MAX(sequence_order), -1) + 1 \n",
    "               FROM messages \n",
    "               WHERE conversation_id = %s\"\"\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        sequence_order = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert the message\n",
    "        message_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Extract fields from message\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        \n",
    "        # Truncate content if too long\n",
    "        if len(content) > MAX_SAFE_LENGTH:\n",
    "            content = content[:MAX_SAFE_LENGTH] + \"\\n\\n[Content truncated due to size limitations]\"\n",
    "            \n",
    "        # Handle tool_calls - only truncate if needed\n",
    "        if \"tool_calls\" in message and message[\"tool_calls\"]:\n",
    "            tool_calls_json = json.dumps(message[\"tool_calls\"])\n",
    "            if len(tool_calls_json) > MAX_SAFE_LENGTH:\n",
    "                # Truncate large tool calls data\n",
    "                tool_calls = json.dumps({\n",
    "                    \"truncated\": True, \n",
    "                    \"message\": \"Tool calls data was too large to store\"\n",
    "                })\n",
    "            else:\n",
    "                tool_calls = tool_calls_json\n",
    "        else:\n",
    "            tool_calls = None\n",
    "            \n",
    "        # Get tool_call_id - no truncation needed with TEXT column\n",
    "        tool_call_id = message.get(\"tool_call_id\")\n",
    "        \n",
    "        # Insert with properly handled content sizes\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO messages \n",
    "               (message_id, conversation_id, role, content, tool_calls, tool_call_id, sequence_order) \n",
    "               VALUES (%s, %s, %s, %s, %s, %s, %s)\"\"\",\n",
    "            (message_id, conversation_id, role, content, tool_calls, tool_call_id, sequence_order)\n",
    "        )\n",
    "        \n",
    "        # Make sure changes are committed\n",
    "        conn.commit()\n",
    "\n",
    "def db_conversation_exists(conversation_id):\n",
    "    \"\"\"Check if a conversation exists - new helper function\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT 1 FROM conversations WHERE conversation_id = %s\", \n",
    "            (conversation_id,)\n",
    "        )\n",
    "        return cursor.fetchone() is not None\n",
    "\n",
    "\n",
    "def db_get_recent_conversations(limit: int = 10):\n",
    "    \"\"\"Get the most recent conversations\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"\"\"SELECT conversation_id, created_at, last_updated \n",
    "               FROM conversations \n",
    "               ORDER BY last_updated DESC \n",
    "               LIMIT %s\"\"\",\n",
    "            (limit,)\n",
    "        )\n",
    "        \n",
    "        conversations = []\n",
    "        for row in cursor.fetchall():\n",
    "            conversations.append({\n",
    "                \"conversation_id\": row[0],\n",
    "                \"created_at\": row[1].isoformat() if row[1] else None,\n",
    "                \"last_updated\": row[2].isoformat() if row[2] else None\n",
    "            })\n",
    "            \n",
    "        return conversations\n",
    "\n",
    "def db_get_conversation_preview(conversation_id: str, message_limit: int = 3):\n",
    "    \"\"\"Get conversation with limited number of most recent messages\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        # First check if conversation exists\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT created_at, last_updated FROM conversations WHERE conversation_id = %s\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        conv_data = cursor.fetchone()\n",
    "        if not conv_data:\n",
    "            return None\n",
    "            \n",
    "        # Get total message count\n",
    "        cursor.execute(\n",
    "            \"SELECT COUNT(*) FROM messages WHERE conversation_id = %s\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        total_messages = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get the most recent messages\n",
    "        cursor.execute(\n",
    "            \"\"\"SELECT role, content, timestamp \n",
    "               FROM messages \n",
    "               WHERE conversation_id = %s \n",
    "               ORDER BY sequence_order DESC\n",
    "               LIMIT %s\"\"\",\n",
    "            (conversation_id, message_limit)\n",
    "        )\n",
    "        \n",
    "        messages = []\n",
    "        for row in cursor.fetchall():\n",
    "            messages.append({\n",
    "                \"role\": row[0],\n",
    "                \"content\": row[1][:150] + \"...\" if len(row[1]) > 150 else row[1],  # Truncate long content\n",
    "                \"timestamp\": row[2].isoformat() if row[2] else None\n",
    "            })\n",
    "        \n",
    "        # Return in correct order (newest last)\n",
    "        messages.reverse()\n",
    "        \n",
    "        return {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"created_at\": conv_data[0].isoformat() if conv_data[0] else None,\n",
    "            \"last_updated\": conv_data[1].isoformat() if conv_data[1] else None,\n",
    "            \"total_messages\": total_messages,\n",
    "            \"recent_messages\": messages\n",
    "        }\n",
    "\n",
    "def db_delete_conversation(conversation_id: str) -> bool:\n",
    "    \"\"\"Delete a conversation and all its messages\"\"\"\n",
    "    with get_db_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Since we don't have CASCADE DELETE due to no foreign keys,\n",
    "        # we need to delete messages first, then conversation\n",
    "        cursor.execute(\n",
    "            \"DELETE FROM messages WHERE conversation_id = %s\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        \n",
    "        cursor.execute(\n",
    "            \"DELETE FROM conversations WHERE conversation_id = %s\",\n",
    "            (conversation_id,)\n",
    "        )\n",
    "        \n",
    "        # Check if deletion was successful\n",
    "        return cursor.rowcount > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0abd4549-9bf0-451b-a559-45f8be4d8056",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Authenticate with Aura and Check LLM status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70cc5149-4b32-4866-86ac-a22ce16ad828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.287972Z",
     "iopub.status.busy": "2025-03-09T02:29:17.287673Z",
     "iopub.status.idle": "2025-03-09T02:29:17.306003Z",
     "shell.execute_reply": "2025-03-09T02:29:17.304794Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.287944Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup the OpenAI client to connect to our custom LLM endpoint\n",
    "client = openai.OpenAI(\n",
    "    api_key=INFERENCE_AUTH_TOKEN,\n",
    "    base_url=INFERENCE_API_ENDPOINT\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7773f454-f0ce-4870-aa16-3edfec797231",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Tool and function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3160e6f-05b3-4ea4-96ac-bc8ed4de7331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.308735Z",
     "iopub.status.busy": "2025-03-09T02:29:17.308167Z",
     "iopub.status.idle": "2025-03-09T02:29:17.318569Z",
     "shell.execute_reply": "2025-03-09T02:29:17.318017Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.308636Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper functions and tools \n",
    "def google_search(parameters):\n",
    "    \"\"\"\n",
    "    Makes a call to SerpAPI to search Google and returns results with sources.\n",
    "    \"\"\"\n",
    "    api_key = SERPAPI_API_KEY\n",
    "    if not api_key:\n",
    "        return {\"content\": \"SERPAPI_API_KEY not set. Please set your search API key.\", \"sources\": []}\n",
    "    \n",
    "    query = parameters.get(\"query\", \"\")\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": api_key,\n",
    "        \"num\": \"5\",  # Request top 5 results for more comprehensive sources\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        sources = []\n",
    "        content_parts = []\n",
    "        \n",
    "        if \"organic_results\" in data and len(data[\"organic_results\"]) > 0:\n",
    "            results = data[\"organic_results\"]\n",
    "            \n",
    "            for i, result in enumerate(results[:5]):  # Limit to top 5 results\n",
    "                snippet = result.get(\"snippet\", \"\")\n",
    "                link = result.get(\"link\", \"\")\n",
    "                title = result.get(\"title\", \"\")\n",
    "                \n",
    "                if snippet and link:\n",
    "                    content_parts.append(f\"{snippet}\")\n",
    "                    sources.append({\"title\": title, \"url\": link})\n",
    "            \n",
    "            content = \"Google Results:\\n\\n\" + \"\\n\\n\".join(content_parts)\n",
    "        else:\n",
    "            content = \"No results found.\"\n",
    "        \n",
    "        return {\"content\": content, \"sources\": sources}\n",
    "    except Exception as e:\n",
    "        return {\"content\": f\"Google search failed: {e}\", \"sources\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8fc081b-a5cf-4312-97cd-d488817a62af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.320242Z",
     "iopub.status.busy": "2025-03-09T02:29:17.319917Z",
     "iopub.status.idle": "2025-03-09T02:29:17.352909Z",
     "shell.execute_reply": "2025-03-09T02:29:17.352379Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.320206Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_singlestore_knowledge(parameters):\n",
    "    query = parameters.get(\"query\", \"\")\n",
    "    if not query:\n",
    "        return {\"content\": \"No query provided for SingleStore knowledge retrieval.\", \"sources\": []}\n",
    "    \n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    timeout = 5\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            headers = {\n",
    "                \"accept\": \"application/json\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {HYBRID_SEARCH_AUTH}\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"query_text\": query,\n",
    "                \"top_k\": 5,\n",
    "                \"vector_weight\": 0.7,\n",
    "                \"text_weight\": 0.3,\n",
    "                \"model_name\": \"all-MiniLM-L6-v2\"\n",
    "            }\n",
    "            \n",
    "            # Increased timeout and improved error handling\n",
    "            response = requests.post(HYBRID_SEARCH_URL, headers=headers, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            sources = []\n",
    "            content_parts = [\"Here are the relevant SingleStore knowledge chunks:\"]\n",
    "            \n",
    "            if \"results\" in data and data[\"results\"]:\n",
    "                for i, result in enumerate(data[\"results\"], 1):\n",
    "                    # Handle array-format results from hybrid search\n",
    "                    if isinstance(result, list) and len(result) >= 5:\n",
    "                        # Extract fields: [doc_id, url, chunk_index, chunk_text, score]\n",
    "                        doc_id = result[0]\n",
    "                        url = result[1]\n",
    "                        chunk_index = result[2]\n",
    "                        text_content = result[3]\n",
    "                        score = result[4]\n",
    "                        \n",
    "                        # Generate a title from the URL\n",
    "                        clean_url = url.replace(\"https://docs.singlestore.com/\", \"\")\n",
    "                        path_parts = [part for part in clean_url.split(\"/\") if part]\n",
    "                        \n",
    "                        if path_parts:\n",
    "                            title = \" \".join(path_parts[-1].split(\"-\")).title()\n",
    "                            if title.endswith(\"/\"):\n",
    "                                title = title[:-1]\n",
    "                        else:\n",
    "                            title = f\"SingleStore Documentation ({i})\"\n",
    "                        \n",
    "                        content_parts.append(f\"[{i}] {text_content}\")\n",
    "                        sources.append({\"title\": title, \"url\": url})\n",
    "                \n",
    "                content = \"\\n\\n\".join(content_parts)\n",
    "            else:\n",
    "                content = \"No relevant information found in the SingleStore knowledge base.\"\n",
    "            \n",
    "            return {\"content\": content, \"sources\": sources}\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            # Specific handling for timeout errors\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            # Fall back to Google search\n",
    "            return {\n",
    "                \"content\": \"The SingleStore knowledge base search timed out. I'll try to search using Google instead.\",\n",
    "                \"sources\": [],\n",
    "                \"fallback_to\": \"google_search\" \n",
    "            }\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            return {\n",
    "                \"content\": f\"Error retrieving information: {str(e)}. I'll try to search using Google instead.\",\n",
    "                \"sources\": [],\n",
    "                \"fallback_to\": \"google_search\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a34f07-9c1a-4751-8b9a-cc3cf7d8de76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.356566Z",
     "iopub.status.busy": "2025-03-09T02:29:17.356204Z",
     "iopub.status.idle": "2025-03-09T02:29:17.383191Z",
     "shell.execute_reply": "2025-03-09T02:29:17.382511Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.356537Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_tool_result(tool_result_obj, tool_used, tool_call_id, conversation_id):\n",
    "    \"\"\"Process and store a tool result\"\"\"\n",
    "    try:\n",
    "        # Process tool result\n",
    "        if isinstance(tool_result_obj, dict) and \"content\" in tool_result_obj:\n",
    "            readable_result = tool_result_obj[\"content\"]\n",
    "            sources = tool_result_obj.get(\"sources\", [])\n",
    "        else:\n",
    "            readable_result = str(tool_result_obj)\n",
    "            sources = extract_sources_from_tool_result(\n",
    "                tool_result_obj,\n",
    "                default_source={\"title\": f\"{tool_used} result\", \"url\": \"\"}\n",
    "            )\n",
    "        \n",
    "        # For very large results, truncate content only\n",
    "        MAX_SAFE_LENGTH = 15 * 1024 * 1024  # 15MB\n",
    "        \n",
    "        # Truncate readable result if needed\n",
    "        if len(readable_result) > MAX_SAFE_LENGTH:\n",
    "            readable_result = readable_result[:MAX_SAFE_LENGTH] + \"\\n\\n[Content truncated due to size limitations]\"\n",
    "            \n",
    "        # Prepare content to store - for very large objects, store a simplified version\n",
    "        if isinstance(tool_result_obj, dict):\n",
    "            try:\n",
    "                json_size = len(json.dumps(tool_result_obj))\n",
    "                if json_size > MAX_SAFE_LENGTH:\n",
    "                    # Create a simplified version with just the content\n",
    "                    simplified_obj = {\n",
    "                        \"content\": readable_result[:MAX_SAFE_LENGTH] if len(readable_result) > MAX_SAFE_LENGTH else readable_result,\n",
    "                        \"sources\": tool_result_obj.get(\"sources\", []),\n",
    "                        \"truncated\": True\n",
    "                    }\n",
    "                    content_to_store = json.dumps(simplified_obj)\n",
    "                else:\n",
    "                    content_to_store = json.dumps(tool_result_obj)\n",
    "            except:\n",
    "                # If JSON serialization fails, use the string representation\n",
    "                content_to_store = str(tool_result_obj)[:MAX_SAFE_LENGTH]\n",
    "        else:\n",
    "            content_to_store = str(tool_result_obj)[:MAX_SAFE_LENGTH]\n",
    "        \n",
    "        # Create tool response message - no truncation for tool_call_id\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call_id,  # No need to truncate with TEXT column\n",
    "            \"content\": content_to_store\n",
    "        }\n",
    "        \n",
    "        # Store tool response in database\n",
    "        db_add_message(conversation_id, tool_message)\n",
    "        \n",
    "        return readable_result, sources\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing tool result: {str(e)}\")\n",
    "        return f\"Error processing tool result: {str(e)}\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08834e48-f0c6-4139-a00f-51664548786a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.384555Z",
     "iopub.status.busy": "2025-03-09T02:29:17.384228Z",
     "iopub.status.idle": "2025-03-09T02:29:17.409095Z",
     "shell.execute_reply": "2025-03-09T02:29:17.408429Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.384510Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Register all the tools\n",
    "tool_registry = {\n",
    "    \"google_search\": google_search,\n",
    "    \"retrieve_singlestore_knowledge\": retrieve_singlestore_knowledge\n",
    "}\n",
    "\n",
    "# Define the tool specifications for OpenAI function calling\n",
    "tool_specs = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google_search\",\n",
    "            \"description\": \"Search the web for general knowledge information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve_singlestore_knowledge\",\n",
    "            \"description\": \"Search the SingleStore knowledge base for information about SingleStore, databases, vector search, or related technology\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query about SingleStore or database technology\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a4d45f9-2de1-4c94-96ba-ac8091b642f8",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Chat Bot building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7224e132-9428-430d-9e58-66b2d7556c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.410540Z",
     "iopub.status.busy": "2025-03-09T02:29:17.410205Z",
     "iopub.status.idle": "2025-03-09T02:29:17.456534Z",
     "shell.execute_reply": "2025-03-09T02:29:17.455760Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.410511Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant. When a user's query requires up-to-date or external information, \n",
    "use the appropriate tool:\n",
    "\n",
    "1. For general knowledge queries, use the 'google_search' function.\n",
    "2. For queries related to SingleStore, databases, vector search, or any SingleStore-specific technology, \n",
    "   use the 'retrieve_singlestore_knowledge' function.\n",
    "\n",
    "After receiving tool results, carefully analyze the information and use it as the primary source for your answer. \n",
    "This information is the most up-to-date and should be prioritized over your pre-existing knowledge.\n",
    "\n",
    "IMPORTANT: Format your response using these guidelines:\n",
    "1. Write your response in Markdown format.\n",
    "2. Include inline citations that combine the source number and direct link, like this: \n",
    "   [[1]](https://example.com). For example: SingleStore supports vectorization[[1]](https://docs.singlestore.com).\n",
    "3. At the end of your response, include a '## Sources' section with a numbered list of all the sources you cited.\n",
    "4. Format each source in the list as a Markdown link with the title as the link text.\n",
    "5. Ensure every significant claim or piece of information has a citation.\n",
    "\n",
    "Example format:\n",
    "SingleStore supports vectorization through its built-in vector functions[[1]](https://docs.singlestore.com/managed-service/en/reference/vector-functions.html). This capability enables efficient \n",
    "similarity search operations for machine learning applications[[2]](https://www.singlestore.com/blog/vector-search-for-ai/).\n",
    "\n",
    "## Sources\n",
    "[1] [SingleStore Vector Search Documentation](https://docs.singlestore.com/managed-service/en/reference/vector-functions.html)\n",
    "[2] [Building AI Applications with SingleStore](https://www.singlestore.com/blog/vector-search-for-ai/)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d0e8d04-f387-4064-9d2f-c19bf3cb195b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.458270Z",
     "iopub.status.busy": "2025-03-09T02:29:17.457971Z",
     "iopub.status.idle": "2025-03-09T02:29:17.504420Z",
     "shell.execute_reply": "2025-03-09T02:29:17.503785Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.458243Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_llm_response(history):\n",
    "    \"\"\"\n",
    "    Uses the OpenAI client to get the assistant's response based on conversation history.\n",
    "    \n",
    "    Args:\n",
    "        history (list): List of message dictionaries with role and content keys\n",
    "    \n",
    "    Returns:\n",
    "        str: The assistant's response content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=INFERENCE_MODEL_NAME,\n",
    "            messages=history,\n",
    "            temperature=MODEL_RESPONSE_TEMPERATURE,\n",
    "            max_tokens=MODEL_RESPONSE_MAX_TOKENS,\n",
    "            tools=tool_specs\n",
    "        )\n",
    "        \n",
    "        if response.choices and len(response.choices) > 0:\n",
    "            message = response.choices[0].message\n",
    "            \n",
    "            # Check if the message includes a function call\n",
    "            if message.tool_calls:\n",
    "                tool_call = message.tool_calls[0]\n",
    "                function_call = {\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"parameters\": json.loads(tool_call.function.arguments)\n",
    "                }\n",
    "                \n",
    "                # Format response as a JSON string for our existing parser\n",
    "                return json.dumps({\"function_call\": function_call})\n",
    "            \n",
    "            return message.content\n",
    "        \n",
    "        return \"No response generated.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting LLM response: {e}\")\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b19c517c-d211-4e0e-8279-3f02a37a6c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.505998Z",
     "iopub.status.busy": "2025-03-09T02:29:17.505718Z",
     "iopub.status.idle": "2025-03-09T02:29:17.550687Z",
     "shell.execute_reply": "2025-03-09T02:29:17.550038Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.505969Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pydantic models for API\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ConversationResponse(BaseModel):\n",
    "    conversation_id: str\n",
    "    messages: List[Message]\n",
    "\n",
    "class MessageRequest(BaseModel):\n",
    "    conversation_id: Optional[str] = None\n",
    "    message: str\n",
    "\n",
    "class MessageResponse(BaseModel):\n",
    "    conversation_id: str\n",
    "    response: str\n",
    "    tool_used: Optional[str] = None\n",
    "    tool_result: Optional[str] = None\n",
    "\n",
    "class MessageStreamRequest(BaseModel):\n",
    "    conversation_id: Optional[str] = None\n",
    "    message: str\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup: Initialize the database\n",
    "    print(\"Initializing database...\")\n",
    "    try:\n",
    "        # Your existing init_database code\n",
    "        init_database()\n",
    "        print(\"Database initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Database initialization error: {e}\")\n",
    "    \n",
    "    yield  # This is where the app runs\n",
    "    \n",
    "    # Shutdown: Clean up resources if needed\n",
    "    print(\"Shutting down...\")\n",
    "    # Any cleanup code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1f002c-7e08-46e6-9749-e9d8d5dfde47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.552306Z",
     "iopub.status.busy": "2025-03-09T02:29:17.551826Z",
     "iopub.status.idle": "2025-03-09T02:29:17.596236Z",
     "shell.execute_reply": "2025-03-09T02:29:17.595504Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.552272Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add streaming versions of get_llm_response\n",
    "async def get_llm_response_stream(history):\n",
    "    \"\"\"\n",
    "    Uses the OpenAI client to get a streaming response based on conversation history.\n",
    "    \n",
    "    Args:\n",
    "        history (list): List of message dictionaries from database\n",
    "    \n",
    "    Returns:\n",
    "        AsyncGenerator: Yields chunks of the assistant's response and a final result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Start the streaming response\n",
    "        stream = await asyncio.to_thread(\n",
    "            lambda: client.chat.completions.create(\n",
    "                model=INFERENCE_MODEL_NAME,\n",
    "                messages=history,\n",
    "                temperature=MODEL_RESPONSE_TEMPERATURE,\n",
    "                max_tokens=MODEL_RESPONSE_MAX_TOKENS,\n",
    "                tools=tool_specs,\n",
    "                stream=True # Enable streaming\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Initialize variables to track the full content and detect tool calls\n",
    "        full_content = \"\"\n",
    "        tool_call_parts = []\n",
    "        is_tool_call = False\n",
    "        \n",
    "        # Process each chunk as it arrives\n",
    "        for chunk in stream:\n",
    "            if not chunk.choices:\n",
    "                continue\n",
    "            \n",
    "            delta = chunk.choices[0].delta\n",
    "            \n",
    "            # Check for tool calls in this chunk\n",
    "            if delta.tool_calls:\n",
    "                is_tool_call = True\n",
    "                # This is a tool call chunk\n",
    "                tool_call = delta.tool_calls[0]\n",
    "                \n",
    "                # Function might be split across chunks, so we collect them\n",
    "                if hasattr(tool_call.function, \"name\") and tool_call.function.name:\n",
    "                    tool_call_parts.append({\"type\": \"name\", \"content\": tool_call.function.name})\n",
    "                    yield json.dumps({\"type\": \"tool_call_start\", \"name\": tool_call.function.name}) + \"\\n\"\n",
    "                \n",
    "                if hasattr(tool_call.function, \"arguments\") and tool_call.function.arguments:\n",
    "                    tool_call_parts.append({\"type\": \"args\", \"content\": tool_call.function.arguments})\n",
    "            elif delta.content:\n",
    "                # Regular content chunk\n",
    "                full_content += delta.content\n",
    "                yield json.dumps({\"type\": \"content\", \"content\": delta.content}) + \"\\n\"\n",
    "        \n",
    "        # If it was a tool call, reconstruct the complete tool call\n",
    "        if is_tool_call:\n",
    "            # Reconstruct the tool call from parts\n",
    "            function_name = next((part[\"content\"] for part in tool_call_parts if part[\"type\"] == \"name\"), \"\")\n",
    "            arguments = \"\".join(part[\"content\"] for part in tool_call_parts if part[\"type\"] == \"args\")\n",
    "            \n",
    "            try:\n",
    "                # Parse the arguments as JSON\n",
    "                args_obj = json.loads(arguments)\n",
    "                \n",
    "                # Yield the complete tool call as a final result\n",
    "                yield json.dumps({\n",
    "                    \"type\": \"final_result\",\n",
    "                    \"is_tool_call\": True,\n",
    "                    \"function\": {\n",
    "                        \"name\": function_name,\n",
    "                        \"parameters\": args_obj\n",
    "                    },\n",
    "                    \"content\": full_content\n",
    "                }) + \"\\n\"\n",
    "            except json.JSONDecodeError:\n",
    "                yield json.dumps({\"type\": \"error\", \"content\": \"Failed to parse tool call arguments\"}) + \"\\n\"\n",
    "                yield json.dumps({\n",
    "                    \"type\": \"final_result\",\n",
    "                    \"is_tool_call\": False,\n",
    "                    \"content\": full_content\n",
    "                }) + \"\\n\"\n",
    "        else:\n",
    "            # Regular response (no tool call)\n",
    "            yield json.dumps({\n",
    "                \"type\": \"final_result\",\n",
    "                \"is_tool_call\": False,\n",
    "                \"content\": full_content\n",
    "            }) + \"\\n\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error getting streaming LLM response: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        yield json.dumps({\"type\": \"error\", \"content\": error_msg}) + \"\\n\"\n",
    "        yield json.dumps({\n",
    "            \"type\": \"final_result\",\n",
    "            \"is_tool_call\": False,\n",
    "            \"content\": error_msg\n",
    "        }) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98838626-069a-4219-9e81-eba25caf8a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.597602Z",
     "iopub.status.busy": "2025-03-09T02:29:17.597174Z",
     "iopub.status.idle": "2025-03-09T02:29:17.641503Z",
     "shell.execute_reply": "2025-03-09T02:29:17.640880Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.597564Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"SingleStore Chatbot API\",\n",
    "    lifespan=lifespan\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3adfc665-46ab-44b4-9bfc-f8dce4006894",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## API endpoints definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aa1ec81-6286-4949-80b9-d32bf4be8005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.643291Z",
     "iopub.status.busy": "2025-03-09T02:29:17.642760Z",
     "iopub.status.idle": "2025-03-09T02:29:17.715705Z",
     "shell.execute_reply": "2025-03-09T02:29:17.715026Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.643259Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint to check if the API is running\"\"\"\n",
    "    return {\"status\": \"ok\", \"message\": \"SingleStore Chatbot API is running\"}\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def dependencies_health_check():\n",
    "    \"\"\"Check if the service and its dependencies are healthy\"\"\"\n",
    "    health_status = {\n",
    "        \"status\": \"healthy\",\n",
    "        \"database\": \"healthy\",\n",
    "        \"llm_service\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Check database connection\n",
    "    try:\n",
    "        with get_db_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT 1\")\n",
    "            cursor.fetchone()\n",
    "    except Exception as e:\n",
    "        health_status[\"database\"] = f\"unhealthy: {str(e)}\"\n",
    "        health_status[\"status\"] = \"degraded\"\n",
    "    \n",
    "    # Check LLM service\n",
    "    try:\n",
    "        response = client.models.list()\n",
    "        if not response:\n",
    "            health_status[\"llm_service\"] = \"unhealthy: no models available\"\n",
    "            health_status[\"status\"] = \"degraded\"\n",
    "    except Exception as e:\n",
    "        health_status[\"llm_service\"] = f\"unhealthy: {str(e)}\"\n",
    "        health_status[\"status\"] = \"degraded\"\n",
    "    \n",
    "    return health_status\n",
    "\n",
    "@app.post(\"/conversation\", response_model=ConversationResponse)\n",
    "async def create_conversation():\n",
    "    \"\"\"Create a new conversation with a unique ID\"\"\"\n",
    "    try:\n",
    "        # Replace in-memory operation with database operation\n",
    "        logger.info(\"Creating new conversation with system message\")\n",
    "        conversation_id = db_create_conversation(SYSTEM_MESSAGE)\n",
    "        \n",
    "        logger.info(f\"Getting messages for conversation {conversation_id}\")\n",
    "        # Get messages from database\n",
    "        messages = db_get_conversation(conversation_id)\n",
    "        \n",
    "        # Return the conversation ID and messages\n",
    "        return ConversationResponse(\n",
    "            conversation_id=conversation_id,\n",
    "            messages=[Message(role=msg[\"role\"], content=msg[\"content\"]) for msg in messages]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating conversation: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=f\"Failed to create conversation: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.get(\"/conversation/{conversation_id}\", response_model=ConversationResponse)\n",
    "async def get_conversation(conversation_id: str):\n",
    "    \"\"\"Get the conversation history for a specific conversation ID\"\"\"\n",
    "    if not db_conversation_exists(conversation_id):\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found in db\")\n",
    "\n",
    "    # Get messages from database\n",
    "    messages = db_get_conversation(conversation_id)\n",
    "    \n",
    "    return ConversationResponse(\n",
    "        conversation_id=conversation_id,\n",
    "        messages=[Message(role=msg[\"role\"], content=msg[\"content\"]) for msg in messages]\n",
    "    )\n",
    "\n",
    "@app.delete(\"/conversation/{conversation_id}\")\n",
    "async def delete_conversation(conversation_id: str):\n",
    "    \"\"\"Delete a conversation and all its messages\"\"\"\n",
    "    success = db_delete_conversation(conversation_id)\n",
    "    if not success:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "    return {\"status\": \"success\", \"message\": \"Conversation deleted\"}\n",
    "\n",
    "@app.get(\"/conversations/recent\")\n",
    "async def get_recent_conversations(limit: int = 10):\n",
    "    \"\"\"Get list of recent conversations\"\"\"\n",
    "    return db_get_recent_conversations(limit)\n",
    "\n",
    "@app.get(\"/conversation/{conversation_id}/preview\")\n",
    "async def get_conversation_preview(conversation_id: str, message_limit: int = 3):\n",
    "    \"\"\"Get conversation with preview of recent messages\"\"\"\n",
    "    preview = db_get_conversation_preview(conversation_id, message_limit)\n",
    "    if not preview:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "    return preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e8d0e5a-3048-4d6e-a2d9-c3583a603d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.716970Z",
     "iopub.status.busy": "2025-03-09T02:29:17.716628Z",
     "iopub.status.idle": "2025-03-09T02:29:17.732650Z",
     "shell.execute_reply": "2025-03-09T02:29:17.732184Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.716953Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@app.post(\"/message\", response_model=MessageResponse)\n",
    "async def send_message(request: MessageRequest):\n",
    "    # Initialize or retrieve conversation\n",
    "    conversation_id = request.conversation_id\n",
    "    if not conversation_id or not db_conversation_exists(conversation_id):\n",
    "        conversation_id = db_create_conversation(SYSTEM_MESSAGE)\n",
    "\n",
    "    # Get current messages from database\n",
    "    history = db_get_conversation(conversation_id)\n",
    "    \n",
    "    # Add user message\n",
    "    user_message = {\"role\": \"user\", \"content\": request.message}\n",
    "    db_add_message(conversation_id, user_message)\n",
    "\n",
    "    # Update history with the new user message\n",
    "    history.append(user_message)\n",
    "    \n",
    "    # Initialize tool tracking variables\n",
    "    tool_used = None\n",
    "    tool_result = None\n",
    "    \n",
    "    try:\n",
    "        # Get initial response\n",
    "        response = client.chat.completions.create(\n",
    "            model=INFERENCE_MODEL_NAME,\n",
    "            messages=history,\n",
    "            temperature=MODEL_RESPONSE_TEMPERATURE,\n",
    "            max_tokens=MODEL_RESPONSE_MAX_TOKENS,\n",
    "            tools=tool_specs\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        \n",
    "        # Check for tool calls\n",
    "        if assistant_message.tool_calls:\n",
    "            tool_call = assistant_message.tool_calls[0]\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            # Set the tool_used variable\n",
    "            tool_used = function_name\n",
    "            \n",
    "            # Create assistant message with tool call - no truncation needed\n",
    "            assistant_with_tool = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_message.content or \"\",\n",
    "                \"tool_calls\": [{\n",
    "                    \"id\": tool_call.id,  # Use full ID without truncation\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": function_name,\n",
    "                        \"arguments\": tool_call.function.arguments\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "            \n",
    "            # Store assistant message in database\n",
    "            db_add_message(conversation_id, assistant_with_tool)\n",
    "            \n",
    "            # Execute tool\n",
    "            if function_name in tool_registry:\n",
    "                # Execute the tool and get the result\n",
    "                tool_result_obj = tool_registry[function_name](function_args)\n",
    "                \n",
    "                # Check if we need to fall back to another tool\n",
    "                if isinstance(tool_result_obj, dict) and \"fallback_to\" in tool_result_obj:\n",
    "                    fallback_tool = tool_result_obj.get(\"fallback_to\")\n",
    "                    if fallback_tool in tool_registry:\n",
    "                        # Execute the fallback tool\n",
    "                        fallback_result = tool_registry[fallback_tool](function_args)\n",
    "                        # Update tool tracking\n",
    "                        tool_used = fallback_tool\n",
    "                        tool_result_obj = fallback_result\n",
    "                \n",
    "                # Process tool result and handle large content\n",
    "                readable_result, sources = process_tool_result(\n",
    "                    tool_result_obj, tool_used, tool_call.id, conversation_id  # Use full ID\n",
    "                )\n",
    "                \n",
    "                # Store the readable result\n",
    "                tool_result = readable_result\n",
    "\n",
    "                # Update history with both messages for final response\n",
    "                history.append(assistant_with_tool)\n",
    "                history.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,  # Use full ID\n",
    "                    \"content\": json.dumps(tool_result_obj) if isinstance(tool_result_obj, dict) else str(tool_result_obj)\n",
    "                })\n",
    "                \n",
    "                # Get final response\n",
    "                final_response = client.chat.completions.create(\n",
    "                    model=INFERENCE_MODEL_NAME,\n",
    "                    messages=history,\n",
    "                    temperature=MODEL_RESPONSE_TEMPERATURE,\n",
    "                    max_tokens=MODEL_RESPONSE_MAX_TOKENS\n",
    "                )\n",
    "                \n",
    "                final_content = final_response.choices[0].message.content\n",
    "                \n",
    "                # Add citation links\n",
    "                if sources:\n",
    "                    final_content = add_citation_numbers(final_content, sources)\n",
    "                \n",
    "                # Store final response in database\n",
    "                db_add_message(conversation_id, {\"role\": \"assistant\", \"content\": final_content})\n",
    "                \n",
    "                return MessageResponse(\n",
    "                    conversation_id=conversation_id,\n",
    "                    response=final_content,\n",
    "                    tool_used=tool_used,\n",
    "                    tool_result=tool_result \n",
    "                )\n",
    "            else:\n",
    "                error_msg = f\"Unknown tool: {function_name}\"\n",
    "                db_add_message(conversation_id, {\"role\": \"assistant\", \"content\": error_msg})\n",
    "                return MessageResponse(conversation_id=conversation_id, response=error_msg)\n",
    "        else:\n",
    "            # No tool call, just return the response\n",
    "            content = assistant_message.content\n",
    "            db_add_message(conversation_id, {\"role\": \"assistant\", \"content\": content})\n",
    "            \n",
    "            return MessageResponse(\n",
    "                conversation_id=conversation_id, \n",
    "                response=content,\n",
    "                tool_used=None,\n",
    "                tool_result=None\n",
    "            )\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        return MessageResponse(\n",
    "            conversation_id=conversation_id, \n",
    "            response=error_msg,\n",
    "            tool_used=tool_used,\n",
    "            tool_result=tool_result\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ef3651f-f107-4ce6-8eb2-152015590278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.734799Z",
     "iopub.status.busy": "2025-03-09T02:29:17.734151Z",
     "iopub.status.idle": "2025-03-09T02:29:17.776592Z",
     "shell.execute_reply": "2025-03-09T02:29:17.776031Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.734751Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_citation_numbers(text, sources):\n",
    "    \"\"\"\n",
    "    Enhances text with inline citations that include both numbers and links.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The response text\n",
    "        sources (list): List of source dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        str: Text with properly formatted citations\n",
    "    \"\"\"\n",
    "    if not sources:\n",
    "        return text\n",
    "        \n",
    "    # Check if text already has a sources section\n",
    "    has_sources_section = re.search(r'##\\s+Sources', text, re.IGNORECASE)\n",
    "    \n",
    "    # If it already has sources section, don't add another one\n",
    "    if has_sources_section:\n",
    "        # Just update the citation links\n",
    "        # Look for simple numbered citations like [1]\n",
    "        citation_matches = re.finditer(r'\\[(\\d+)\\]', text)\n",
    "        \n",
    "        # Process from end to beginning to avoid index issues\n",
    "        matches = list(citation_matches)\n",
    "        for match in reversed(matches):\n",
    "            try:\n",
    "                num = int(match.group(1))\n",
    "                if 1 <= num <= len(sources):\n",
    "                    source = sources[num-1]\n",
    "                    url = source.get('url', '')\n",
    "                    if url and not re.search(r'\\[\\[' + str(num) + r'\\]\\]', text):\n",
    "                        start, end = match.span()\n",
    "                        text = text[:start] + f\"[[{num}]]({url})\" + text[end:]\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return text\n",
    "    \n",
    "    # If no sources section, add one with formatted citations\n",
    "    text += \"\\n\\n## Sources\\n\"\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        title = source.get('title', f'Source {i}')\n",
    "        url = source.get('url', '')\n",
    "        text += f\"[{i}] [{title}]({url})\\n\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def extract_sources_from_tool_result(tool_result, default_source=None):\n",
    "    \"\"\"\n",
    "    Extracts sources from various tool result formats.\n",
    "    \n",
    "    Args:\n",
    "        tool_result: The result object from a tool call\n",
    "        default_source: Optional default source if none found\n",
    "    \n",
    "    Returns:\n",
    "        list: List of source dictionaries\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    \n",
    "    # Handle dictionary format with explicit sources key\n",
    "    if isinstance(tool_result, dict) and \"sources\" in tool_result:\n",
    "        return tool_result[\"sources\"]\n",
    "    \n",
    "    # Handle string results that might contain URLs\n",
    "    if isinstance(tool_result, str):\n",
    "        # Extract URLs from the text\n",
    "        urls = re.findall(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', tool_result)\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            sources.append({\n",
    "                \"title\": f\"Source {i} from tool result\",\n",
    "                \"url\": url\n",
    "            })\n",
    "    \n",
    "    # Add default source if provided and no sources found\n",
    "    if not sources and default_source:\n",
    "        sources.append(default_source)\n",
    "    \n",
    "    return sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d244ed9-abe7-43f8-96ff-34a40b6825d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.778011Z",
     "iopub.status.busy": "2025-03-09T02:29:17.777686Z",
     "iopub.status.idle": "2025-03-09T02:29:17.827182Z",
     "shell.execute_reply": "2025-03-09T02:29:17.826544Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.777985Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@app.post(\"/message/stream\")\n",
    "async def send_message_stream(request: MessageStreamRequest):\n",
    "    # Initialize or retrieve conversation\n",
    "    conversation_id = request.conversation_id\n",
    "    if not conversation_id or not db_conversation_exists(conversation_id):\n",
    "        conversation_id = db_create_conversation(SYSTEM_MESSAGE)\n",
    "    \n",
    "    # Add user message to database\n",
    "    user_message = {\"role\": \"user\", \"content\": request.message}\n",
    "    db_add_message(conversation_id, user_message)\n",
    "    \n",
    "    # Get current conversation history from database\n",
    "    history = db_get_conversation(conversation_id)\n",
    "    \n",
    "    async def stream_generator():\n",
    "        yield json.dumps({\"type\": \"info\", \"content\": \"Processing your request...\"}) + \"\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Initialize tracking variables\n",
    "            tool_used = None\n",
    "            tool_result = None\n",
    "            sources = []\n",
    "            \n",
    "            # Stream the initial response and check for tool calls\n",
    "            final_chunk = None\n",
    "            \n",
    "            async for chunk in get_llm_response_stream(history):\n",
    "                # Always forward chunks to client\n",
    "                yield chunk\n",
    "                \n",
    "                try:\n",
    "                    chunk_data = json.loads(chunk.strip())\n",
    "                    if chunk_data.get(\"type\") == \"final_result\":\n",
    "                        final_chunk = chunk_data\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # Check if there's a tool call\n",
    "            if final_chunk and final_chunk.get(\"is_tool_call\", False):\n",
    "                function_info = final_chunk.get(\"function\", {})\n",
    "                function_name = function_info.get(\"name\")\n",
    "                parameters = function_info.get(\"parameters\", {})\n",
    "                \n",
    "                if function_name and function_name in tool_registry:\n",
    "                    # Track the tool call\n",
    "                    tool_used = function_name\n",
    "                    \n",
    "                    # Generate a tool call ID\n",
    "                    tool_call_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    # Store assistant's request for a tool to database\n",
    "                    assistant_message = {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": final_chunk.get(\"content\", \"\"),\n",
    "                        \"tool_calls\": [\n",
    "                            {\n",
    "                                \"id\": tool_call_id,\n",
    "                                \"type\": \"function\",\n",
    "                                \"function\": {\n",
    "                                    \"name\": function_name,\n",
    "                                    \"arguments\": json.dumps(parameters)\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    db_add_message(conversation_id, assistant_message)\n",
    "                    \n",
    "                    # Notify client about tool execution\n",
    "                    yield json.dumps({\n",
    "                        \"type\": \"tool_calling\",\n",
    "                        \"tool\": function_name,\n",
    "                        \"parameters\": parameters\n",
    "                    }) + \"\\n\"\n",
    "                    \n",
    "                    try:\n",
    "                        # Execute the tool\n",
    "                        yield json.dumps({\"type\": \"tool_execution_start\"}) + \"\\n\"\n",
    "                        tool_result_obj = await asyncio.to_thread(tool_registry[function_name], parameters)\n",
    "                        \n",
    "                        # Check for fallback\n",
    "                        if isinstance(tool_result_obj, dict) and \"fallback_to\" in tool_result_obj:\n",
    "                            fallback_tool = tool_result_obj.get(\"fallback_to\")\n",
    "                            if fallback_tool in tool_registry:\n",
    "                                # Notify client about fallback\n",
    "                                yield json.dumps({\n",
    "                                    \"type\": \"tool_fallback\",\n",
    "                                    \"from\": function_name,\n",
    "                                    \"to\": fallback_tool,\n",
    "                                    \"reason\": tool_result_obj.get(\"content\", \"\")\n",
    "                                }) + \"\\n\"\n",
    "                                \n",
    "                                # Execute fallback tool\n",
    "                                tool_used = fallback_tool\n",
    "                                tool_result_obj = await asyncio.to_thread(tool_registry[fallback_tool], parameters)\n",
    "                        \n",
    "                        # Process the tool result\n",
    "                        if isinstance(tool_result_obj, dict) and \"content\" in tool_result_obj:\n",
    "                            tool_result = tool_result_obj[\"content\"]\n",
    "                            sources = tool_result_obj.get(\"sources\", [])\n",
    "                        else:\n",
    "                            tool_result = str(tool_result_obj)\n",
    "                            sources = extract_sources_from_tool_result(\n",
    "                                tool_result_obj,\n",
    "                                default_source={\"title\": f\"{tool_used} result\", \"url\": \"\"}\n",
    "                            )\n",
    "                        \n",
    "                        # Add tool result to database\n",
    "                        tool_message = {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call_id,\n",
    "                            \"content\": json.dumps(tool_result_obj) if isinstance(tool_result_obj, dict) else str(tool_result_obj)\n",
    "                        }\n",
    "                        db_add_message(conversation_id, tool_message)\n",
    "                        \n",
    "                        # Send tool result to client\n",
    "                        yield json.dumps({\n",
    "                            \"type\": \"tool_result\",\n",
    "                            \"result\": tool_result,\n",
    "                            \"sources\": sources\n",
    "                        }) + \"\\n\"\n",
    "                        \n",
    "                        # Get the updated history including the tool result\n",
    "                        updated_history = db_get_conversation(conversation_id)\n",
    "                        \n",
    "                        # Get final response with the tool result\n",
    "                        yield json.dumps({\"type\": \"generating_final_response\"}) + \"\\n\"\n",
    "                        \n",
    "                        # Stream the final response\n",
    "                        final_content = \"\"\n",
    "                        async for final_chunk in get_llm_response_stream(updated_history):\n",
    "                            # Forward all chunks to client\n",
    "                            yield final_chunk\n",
    "                            \n",
    "                            try:\n",
    "                                final_data = json.loads(final_chunk.strip())\n",
    "                                if final_data.get(\"type\") == \"content\":\n",
    "                                    final_content += final_data.get(\"content\", \"\")\n",
    "                                elif final_data.get(\"type\") == \"final_result\":\n",
    "                                    if not final_content:\n",
    "                                        final_content = final_data.get(\"content\", \"\")\n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "                        \n",
    "                        # Add citation links to the content\n",
    "                        if sources:\n",
    "                            enhanced_content = add_citation_numbers(final_content, sources)\n",
    "                            if enhanced_content != final_content:\n",
    "                                # Send enhanced content with citations\n",
    "                                yield json.dumps({\n",
    "                                    \"type\": \"enhanced_content\",\n",
    "                                    \"content\": enhanced_content\n",
    "                                }) + \"\\n\"\n",
    "                                final_content = enhanced_content\n",
    "                        \n",
    "                        # Save to database\n",
    "                        db_add_message(conversation_id, {\n",
    "                            \"role\": \"assistant\", \n",
    "                            \"content\": final_content\n",
    "                        })\n",
    "                        \n",
    "                        # Complete the stream\n",
    "                        yield json.dumps({\n",
    "                            \"type\": \"done\", \n",
    "                            \"tool_used\": tool_used,\n",
    "                            \"sources\": sources\n",
    "                        }) + \"\\n\"\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Error executing tool {function_name}: {str(e)}\"\n",
    "                        yield json.dumps({\"type\": \"error\", \"content\": error_msg}) + \"\\n\"\n",
    "                        yield json.dumps({\"type\": \"done\", \"error\": True}) + \"\\n\"\n",
    "                else:\n",
    "                    error_msg = f\"Unknown tool: {function_name}\"\n",
    "                    yield json.dumps({\"type\": \"error\", \"content\": error_msg}) + \"\\n\"\n",
    "                    yield json.dumps({\"type\": \"done\", \"error\": True}) + \"\\n\"\n",
    "            elif final_chunk:\n",
    "                # No tool call, just a regular response\n",
    "                final_content = final_chunk.get(\"content\", \"\")\n",
    "                \n",
    "                # Save to database\n",
    "                db_add_message(conversation_id, {\"role\": \"assistant\", \"content\": final_content})\n",
    "                \n",
    "                # Complete the stream\n",
    "                yield json.dumps({\n",
    "                    \"type\": \"done\", \n",
    "                    \"tool_used\": None,\n",
    "                    \"sources\": []\n",
    "                }) + \"\\n\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in stream processing: {str(e)}\"\n",
    "            yield json.dumps({\"type\": \"error\", \"content\": error_msg}) + \"\\n\"\n",
    "            yield json.dumps({\"type\": \"done\", \"error\": True}) + \"\\n\"\n",
    "    \n",
    "    # Return a properly configured StreamingResponse\n",
    "    return StreamingResponse(stream_generator(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "868524b7-9b57-479f-aa85-20c9861a2fc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:17.828818Z",
     "iopub.status.busy": "2025-03-09T02:29:17.828488Z",
     "iopub.status.idle": "2025-03-09T02:29:18.015912Z",
     "shell.execute_reply": "2025-03-09T02:29:18.015350Z",
     "shell.execute_reply.started": "2025-03-09T02:29:17.828786Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Database tables were created separately, skipping initialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing database...\n",
      "Database initialized successfully\n",
      "Cloud function available at https://apps.aws-virginia-nb2.svc.singlestore.com:8000/notebooks/InteractiveNotebook/62c2d0f3-1e14-45dc-9b1b-67ad645b5d3f/app/docs?authToken=eyJhbGciOiJFUzUxMiIsImtpZCI6IjhhNmVjNWFmLThlNWEtNDQxOS04NmM4LWRkMDkxN2U1YWNlMSIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI1YjQ1OTgxYy04YjA5LTRlYWQtYmVjMC0wOTU0N2Q3YjlhOTciLCJhdWQiOlsibm92YXB1YmxpYyJdLCJleHAiOjE3NDE0ODY5NzIsIm5iZiI6MTc0MTQ4NjY3MiwiaWF0IjoxNzQxNDg2NjcyLCJqdGkiOiI1YWQ1MTk0Yi0xNzM3LTRlMmMtYmE5MS1kYTQ3NTcyZmFjMzAiLCJjb250YWluZXJJRCI6IjYyYzJkMGYzLTFlMTQtNDVkYy05YjFiLTY3YWQ2NDViNWQzZiJ9.ABiy3e5kdfIkRUi4KYwfhYwaoUdqTAwjLg4OicCPUF8RPdPZf9cw2iAUbCgAWp2DdUdRuRzGtBtjLlAZBu3eB4tOAWjItmaJxjDDtz3tQBticCSwondVVCOFnx2tv8ZgLXiPEJoKyFP244RVggTWIRNQQp66xx_9bsWQ9scpj2WF3d8b\n"
     ]
    }
   ],
   "source": [
    "# Add a global exception handler\n",
    "@app.exception_handler(Exception)\n",
    "async def global_exception_handler(request: Request, exc: Exception):\n",
    "    logger.error(f\"Unhandled exception: {str(exc)}\", exc_info=True)\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"message\": f\"Internal Server Error: {str(exc)}\"}\n",
    "    )\n",
    "\n",
    "# run the Cloud function\n",
    "connection_info = await apps.run_function_app(app)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46839e97-c5b6-48d4-909a-f4e003363e16",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# TESTS for all endpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c77f5ff5-918e-4d47-8907-e9019db162dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T02:29:18.017595Z",
     "iopub.status.busy": "2025-03-09T02:29:18.017151Z",
     "iopub.status.idle": "2025-03-09T02:29:18.040391Z",
     "shell.execute_reply": "2025-03-09T02:29:18.039566Z",
     "shell.execute_reply.started": "2025-03-09T02:29:18.017563Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test the root endpoint\n",
    "async def test_root_endpoint():\n",
    "    print(\"\\n===== Testing ROOT ENDPOINT =====\")\n",
    "    try:\n",
    "        result = await root()\n",
    "        print(f\"Response: {json.dumps(result, indent=2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test the create conversation endpoint\n",
    "async def test_create_conversation():\n",
    "    print(\"\\n===== Testing CREATE CONVERSATION ENDPOINT =====\")\n",
    "    try:\n",
    "        result = await create_conversation()\n",
    "        print(f\"Conversation ID: {result.conversation_id}\")\n",
    "        print(f\"Initial messages: {len(result.messages)}\")\n",
    "        for msg in result.messages:\n",
    "            print(f\"  - {msg.role}: {msg.content[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test get conversation endpoint with valid and invalid IDs\n",
    "async def test_get_conversation():\n",
    "    print(\"\\n===== Testing GET CONVERSATION ENDPOINT =====\")\n",
    "    \n",
    "    # First create a conversation to get a valid ID\n",
    "    try:\n",
    "        new_conv = await create_conversation()\n",
    "        valid_id = new_conv.conversation_id\n",
    "        print(f\"Created test conversation with ID: {valid_id}\")\n",
    "        \n",
    "        # Test with valid ID\n",
    "        print(\"\\nTesting with valid conversation ID:\")\n",
    "        result = await get_conversation(valid_id)\n",
    "        print(f\"Retrieved conversation: {result.conversation_id}\")\n",
    "        print(f\"Messages: {len(result.messages)}\")\n",
    "        \n",
    "        # Test with invalid ID\n",
    "        print(\"\\nTesting with invalid conversation ID:\")\n",
    "        try:\n",
    "            invalid_id = str(uuid.uuid4())\n",
    "            result = await get_conversation(invalid_id)\n",
    "            print(f\"Unexpected success with invalid ID: {result}\")\n",
    "        except HTTPException as e:\n",
    "            print(f\"Expected error: {e.status_code} - {e.detail}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test the send message endpoint\n",
    "async def test_send_message():\n",
    "    print(\"\\n===== Testing SEND MESSAGE ENDPOINT =====\")\n",
    "    \n",
    "    # Test with new conversation (no conversation_id)\n",
    "    try:\n",
    "        print(\"\\nTesting message with new conversation:\")\n",
    "        request = MessageRequest(message=\"Tell me about workspaces in SingleStore\")\n",
    "        result = await send_message(request)\n",
    "        print(f\"Conversation ID: {result.conversation_id}\")\n",
    "        print(f\"Response: {result.response[:150]}...\")\n",
    "        if result.tool_used:\n",
    "            print(f\"Tool used: {result.tool_used}\")\n",
    "            print(f\"Tool result preview: {result.tool_result[:150] if result.tool_result else 'None'}...\")\n",
    "        \n",
    "        # Test with existing conversation\n",
    "        print(\"\\nTesting message with existing conversation:\")\n",
    "        request = MessageRequest(\n",
    "            conversation_id=result.conversation_id,\n",
    "            message=\"Please provide more details about IVF_PQFS indexes\"\n",
    "        )\n",
    "        result = await send_message(request)\n",
    "        print(f\"Conversation ID: {result.conversation_id}\")\n",
    "        print(f\"Response: {result.response[:150]}...\")\n",
    "        if result.tool_used:\n",
    "            print(f\"Tool used: {result.tool_used}\")\n",
    "            print(f\"Tool result preview: {result.tool_result[:150] if result.tool_result else 'None'}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test recent conversations endpoint\n",
    "async def test_recent_conversations():\n",
    "    print(\"\\n===== Testing RECENT CONVERSATIONS ENDPOINT =====\")\n",
    "    try:\n",
    "        # First create a few conversations to ensure we have data\n",
    "        for i in range(3):\n",
    "            await create_conversation()\n",
    "            \n",
    "        # Now test the endpoint\n",
    "        result = await get_recent_conversations(limit=5)\n",
    "        print(f\"Retrieved {len(result)} recent conversations\")\n",
    "        \n",
    "        # Display each conversation details\n",
    "        for i, conv in enumerate(result):\n",
    "            print(f\"\\nConversation {i+1}:\")\n",
    "            print(f\"  ID: {conv['conversation_id']}\")\n",
    "            print(f\"  Created: {conv['created_at']}\")\n",
    "            print(f\"  Updated: {conv['last_updated']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test conversation preview endpoint\n",
    "async def test_conversation_preview():\n",
    "    print(\"\\n===== Testing CONVERSATION PREVIEW ENDPOINT =====\")\n",
    "    try:\n",
    "        # Create a conversation and add messages\n",
    "        new_conv = await create_conversation()\n",
    "        conv_id = new_conv.conversation_id\n",
    "        print(f\"Created conversation with ID: {conv_id}\")\n",
    "        \n",
    "        # Add a couple of messages\n",
    "        request1 = MessageRequest(conversation_id=conv_id, message=\"Tell me about SingleStore\")\n",
    "        await send_message(request1)\n",
    "        \n",
    "        request2 = MessageRequest(conversation_id=conv_id, message=\"How does vector search work?\")\n",
    "        await send_message(request2)\n",
    "        \n",
    "        # Test the preview endpoint\n",
    "        print(\"\\nTesting preview with valid conversation ID:\")\n",
    "        preview = await get_conversation_preview(conv_id, message_limit=2)\n",
    "        print(f\"Conversation ID: {preview['conversation_id']}\")\n",
    "        print(f\"Total messages: {preview['total_messages']}\")\n",
    "        print(f\"Recent messages: {len(preview['recent_messages'])}\")\n",
    "        \n",
    "        for i, msg in enumerate(preview['recent_messages']):\n",
    "            print(f\"\\nMessage {i+1}:\")\n",
    "            print(f\"  Role: {msg['role']}\")\n",
    "            print(f\"  Content: {msg['content'][:50]}...\")\n",
    "            print(f\"  Timestamp: {msg['timestamp']}\")\n",
    "        \n",
    "        # Test with invalid ID\n",
    "        print(\"\\nTesting preview with invalid conversation ID:\")\n",
    "        try:\n",
    "            invalid_id = str(uuid.uuid4())\n",
    "            await get_conversation_preview(invalid_id)\n",
    "            print(\"Unexpected success with invalid ID\")\n",
    "        except HTTPException as e:\n",
    "            print(f\"Expected error: {e.status_code} - {e.detail}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test delete conversation endpoint\n",
    "async def test_delete_conversation():\n",
    "    print(\"\\n===== Testing DELETE CONVERSATION ENDPOINT =====\")\n",
    "    try:\n",
    "        # Create a conversation to delete\n",
    "        new_conv = await create_conversation()\n",
    "        conv_id = new_conv.conversation_id\n",
    "        print(f\"Created conversation with ID: {conv_id}\")\n",
    "        \n",
    "        # Delete the conversation\n",
    "        print(\"\\nDeleting the conversation:\")\n",
    "        result = await delete_conversation(conv_id)\n",
    "        print(f\"Result: {result}\")\n",
    "        \n",
    "        # Verify it's been deleted\n",
    "        print(\"\\nTrying to access the deleted conversation:\")\n",
    "        try:\n",
    "            await get_conversation(conv_id)\n",
    "            print(\"Unexpected success accessing deleted conversation\")\n",
    "        except HTTPException as e:\n",
    "            print(f\"Expected error: {e.status_code} - {e.detail}\")\n",
    "        \n",
    "        # Test with invalid ID\n",
    "        print(\"\\nTesting delete with invalid conversation ID:\")\n",
    "        try:\n",
    "            invalid_id = str(uuid.uuid4())\n",
    "            await delete_conversation(invalid_id)\n",
    "            print(\"Unexpected success with invalid ID\")\n",
    "        except HTTPException as e:\n",
    "            print(f\"Expected error: {e.status_code} - {e.detail}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test health check endpoint\n",
    "async def test_health_endpoint():\n",
    "    print(\"\\n===== Testing HEALTH CHECK ENDPOINT =====\")\n",
    "    try:\n",
    "        result = await health_dep_check()\n",
    "        print(f\"Health status: {result['status']}\")\n",
    "        print(f\"Database: {result['database']}\")\n",
    "        print(f\"LLM service: {result['llm_service']}\")\n",
    "        print(f\"Timestamp: {result['timestamp']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Test function for stream debugging in notebooks\n",
    "async def test_stream_endpoint():\n",
    "    test_request = MessageStreamRequest(\n",
    "        conversation_id=None,\n",
    "        message=\"How to create a vector index on singlestore table\"\n",
    "    )\n",
    "    \n",
    "    response = await send_message_stream(test_request)\n",
    "    \n",
    "    print(\"Stream response chunks:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    async for chunk in response.body_iterator:\n",
    "        # Handle both bytes and string chunks\n",
    "        if isinstance(chunk, bytes):\n",
    "            chunk_str = chunk.decode('utf-8').strip()\n",
    "        else:\n",
    "            chunk_str = str(chunk).strip()\n",
    "        \n",
    "        if not chunk_str:\n",
    "            continue\n",
    "        \n",
    "        print(f\"CHUNK: {chunk_str}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(chunk_str)\n",
    "            if data.get(\"type\") == \"tool_result\":\n",
    "                print(\"\\nTOOL RESULT DETECTED:\")\n",
    "                print(f\"Tool Used: {data.get('tool', 'unknown')}\")\n",
    "                print(f\"Sources: {len(data.get('sources', []))}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"Stream complete\")\n",
    "\n",
    "# Run all tests sequentially\n",
    "async def run_all_tests():\n",
    "    await test_root_endpoint()\n",
    "    await test_create_conversation()\n",
    "    await test_get_conversation()\n",
    "    await test_send_message()\n",
    "    await test_stream_endpoint()\n",
    "    await test_recent_conversations()\n",
    "    await test_conversation_preview()\n",
    "    await test_delete_conversation()\n",
    "    await test_health_endpoint()\n",
    "\n",
    "# Execute in the notebook\n",
    "# await run_all_tests()"
   ]
  }
 ],
 "metadata": {
  "jupyterlab": {
   "notebooks": {
    "version_major": 6,
    "version_minor": 4
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "singlestore_cell_default_language": "python",
  "singlestore_connection": {
   "connectionID": "",
   "defaultDatabase": ""
  },
  "singlestore_row_limit": 300
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
