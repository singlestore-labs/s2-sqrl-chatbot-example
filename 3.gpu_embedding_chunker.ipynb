{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ec80cce-5f80-4a57-ae3f-88d871565f07",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Text Chunking\n",
    "\n",
    "1. Connects to Given Database\n",
    "2. User selects the chunking algo, embedding model and other parameters\n",
    "3. Defaults to process_documents(batch_size=100, max_workers=16, model_name='all-MiniLM-L6-v2', chunk_size=512, chunk_overlap=50)\n",
    "4. Program creates the necessary table for destination and also creates a checkpoint table for resuming incase the operation fails cause of DB connectivity issues. Rerunning this notebook automatically resumes the previous progress. If no new documents are present then processing ends\n",
    "5. Varying the chunking algorithms from the one available from Langchain.\n",
    "6. Both Fulltext and Vector index (ANN) are created upon completion\n",
    "7. Hybrid search example provided at the end.\n",
    "8. For fastest results run in a GPU container with parallelism options enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ea91e-835c-4e0f-b976-16bfcbda0684",
   "metadata": {
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip -q install langchain langchain-text-splitters sentence_transformers torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57619dd1-a79e-4ef7-8764-e14dac796e20",
   "metadata": {
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  \n",
    "import multiprocessing\n",
    "import singlestoredb as s2\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "#########################################\n",
    "#  Helper Functions for DB Connections  #\n",
    "#########################################\n",
    "def connect_to_db(database_name='knowlagent'):\n",
    "    \"\"\"Return a new SingleStore DB connection.\"\"\"\n",
    "    return s2.connect(database=database_name)\n",
    "\n",
    "#########################################\n",
    "#  Table Creation & Clearing Functions  #\n",
    "#########################################\n",
    "def create_tables(cursor, model_name):\n",
    "    \"\"\"\n",
    "    Create the main chunks table and a checkpoint table.\n",
    "    Note: The main table is created as a columnstore table so that the FULLTEXT\n",
    "    index needed for hybrid search will work.\n",
    "    \"\"\"\n",
    "    table_name = f\"s2docs_chunks_{model_name.replace('-', '_').replace('/', '_')}\"\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "       doc_id BIGINT NOT NULL,\n",
    "       source_url TEXT,\n",
    "       chunk_index INT NOT NULL,\n",
    "       chunk_text LONGTEXT,\n",
    "       embedding JSON DEFAULT NULL,\n",
    "       vector_embedding VECTOR(384) DEFAULT NULL,\n",
    "       SORT KEY (doc_id, chunk_index)\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "    \n",
    "    checkpoint_table = f\"{table_name}_checkpoint\"\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE ROWSTORE TABLE IF NOT EXISTS {checkpoint_table} (\n",
    "            id INT PRIMARY KEY,\n",
    "            last_processed_id BIGINT,\n",
    "            timestamp DATETIME\n",
    "        )\n",
    "    \"\"\")\n",
    "    return table_name, checkpoint_table\n",
    "\n",
    "def create_indexes(cursor, table_name):\n",
    "    \"\"\"Create vector and fulltext indexes on a columnstore table in SingleStore.\"\"\"\n",
    "    # Create the vector index using ANN.\n",
    "    try:\n",
    "        cursor.execute(f\"CREATE INDEX idx_{table_name}_vector ON {table_name}(vector_embedding) USING ANN;\")\n",
    "        print(\"Vector index created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create vector index: {e}\")\n",
    "        \n",
    "    # Create the fulltext index using ALTER TABLE.\n",
    "    try:\n",
    "        cursor.execute(f\"ALTER TABLE {table_name} ADD FULLTEXT INDEX idx_{table_name}_text (chunk_text);\")\n",
    "        print(\"FULLTEXT index created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create FULLTEXT index: {e}\")\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "#  Checkpoint Functions                 #\n",
    "#########################################\n",
    "def get_checkpoint(cursor, checkpoint_table):\n",
    "    \"\"\"Retrieve the last-processed document ID from the checkpoint table.\"\"\"\n",
    "    cursor.execute(f\"SELECT last_processed_id FROM {checkpoint_table} WHERE id = 1\")\n",
    "    checkpoint = cursor.fetchone()\n",
    "    return checkpoint[0] if checkpoint else 0\n",
    "\n",
    "def update_checkpoint(checkpoint_table, last_processed_id):\n",
    "    \"\"\"Update the checkpoint table with the latest processed doc id using a fresh connection.\"\"\"\n",
    "    conn = connect_to_db()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO {checkpoint_table} (id, last_processed_id, timestamp)\n",
    "                VALUES (1, %s, NOW())\n",
    "                ON DUPLICATE KEY UPDATE \n",
    "                    last_processed_id = VALUES(last_processed_id),\n",
    "                    timestamp = VALUES(timestamp)\n",
    "            \"\"\", (last_processed_id,))\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(\"Error updating checkpoint:\", e)\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "#########################################\n",
    "#  Query Functions for Reading Docs      #\n",
    "#########################################\n",
    "def get_unprocessed_docs_count(cursor, table_name, last_processed_id):\n",
    "    \"\"\"Return the count of documents (from s2docs) that need processing.\"\"\"\n",
    "    count_query = f\"\"\"\n",
    "        SELECT COUNT(*) FROM s2docs\n",
    "        WHERE md_content_cleaned IS NOT NULL \n",
    "          AND md_content_cleaned != ''\n",
    "          AND id > {last_processed_id}\n",
    "          AND id NOT IN (SELECT doc_id FROM {table_name})\n",
    "    \"\"\"\n",
    "    cursor.execute(count_query)\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def get_unprocessed_docs_batch(cursor, table_name, last_processed_id, batch_size):\n",
    "    \"\"\"Retrieve a batch of unprocessed docs from s2docs.\"\"\"\n",
    "    select_sql = f\"\"\"\n",
    "        SELECT id, md_content_cleaned\n",
    "        FROM s2docs\n",
    "        WHERE md_content_cleaned IS NOT NULL\n",
    "          AND md_content_cleaned != ''\n",
    "          AND id > %s\n",
    "          AND id NOT IN (SELECT doc_id FROM {table_name})\n",
    "        ORDER BY id\n",
    "        LIMIT %s\n",
    "    \"\"\"\n",
    "    cursor.execute(select_sql, (last_processed_id, batch_size))\n",
    "    return cursor.fetchall()\n",
    "\n",
    "#########################################\n",
    "#  Embedding & Text Processing Functions\n",
    "#########################################\n",
    "def batch_encode_text(model, texts, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Encode a list of texts in batches, using GPU if available.\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        embeddings = model.encode(batch, convert_to_tensor=True)\n",
    "        if torch.is_tensor(embeddings):\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "    if all_embeddings:\n",
    "        return np.vstack(all_embeddings)\n",
    "    return np.array([])\n",
    "\n",
    "def process_document_batch(doc_batch, text_splitter, embed_model):\n",
    "    \"\"\"\n",
    "    Process a batch of documents:\n",
    "     - Split each document's cleaned text into chunks.\n",
    "     - Compute embeddings for each chunk.\n",
    "     - Return a list of tuples (doc_id, chunk_index, chunk_text, embedding_list).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for doc in doc_batch:\n",
    "        doc_id, cleaned_text = doc\n",
    "        chunks = text_splitter.split_text(cleaned_text)\n",
    "        if chunks:\n",
    "            embeddings = batch_encode_text(embed_model, chunks)\n",
    "            for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                results.append((doc_id, idx, chunk, embedding.tolist()))\n",
    "    return results\n",
    "\n",
    "#########################################\n",
    "#  DB Write Functions                   #\n",
    "#########################################\n",
    "def insert_chunks(table_name, chunks_data):\n",
    "    \"\"\"\n",
    "    Insert chunk records into the main table using its own DB connection.\n",
    "    Uses SingleStore's JSON_ARRAY_PACK to insert vector_embedding.\n",
    "    \"\"\"\n",
    "    conn = connect_to_db()\n",
    "    success_count = 0\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO {table_name} (doc_id, source_url, chunk_index, chunk_text, embedding, vector_embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s, JSON_ARRAY_PACK(%s))\n",
    "            \"\"\"\n",
    "            for doc_id,source_url, idx, chunk, embedding in chunks_data:\n",
    "                embedding_json = json.dumps(embedding)\n",
    "                try:\n",
    "                    cursor.execute(insert_sql, (doc_id,source_url, idx, chunk, embedding_json, embedding_json))\n",
    "                    success_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting chunk {idx} for doc {doc_id}: {e}\")\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(\"DB Insert error:\", e)\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return success_count\n",
    "\n",
    "#########################################\n",
    "#   Main Process Function               #\n",
    "#########################################\n",
    "def process_documents(batch_size=100, max_workers=4, model_name='all-MiniLM-L6-v2', chunk_size=512, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Process documents from s2docs:\n",
    "      - Create necessary tables and indexes.\n",
    "      - Read unprocessed documents in batches (using a fresh read connection per batch).\n",
    "      - Process text splitting and compute embeddings using parallel threads.\n",
    "      - Insert chunks using separate DB connections.\n",
    "      - Update checkpoints so that if the process stops, it may resume later.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    embed_model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Setup tables and get checkpoint info.\n",
    "    conn_setup = connect_to_db()\n",
    "    try:\n",
    "        with conn_setup.cursor() as cursor:\n",
    "            table_name, checkpoint_table = create_tables(cursor, model_name)\n",
    "            conn_setup.commit()\n",
    "            last_processed_id = get_checkpoint(cursor, checkpoint_table)\n",
    "            total_docs = get_unprocessed_docs_count(cursor, table_name, last_processed_id)\n",
    "    except Exception as e:\n",
    "        print(\"Error during setup:\", e)\n",
    "        conn_setup.rollback()\n",
    "        conn_setup.close()\n",
    "        return\n",
    "    conn_setup.close()\n",
    "    \n",
    "    if total_docs == 0:\n",
    "        print(\"No new documents to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {total_docs} unprocessed documents.\")\n",
    "    pbar = tqdm(total=total_docs, desc=\"Processing documents\", unit=\"doc\")\n",
    "    \n",
    "    # Initialize text splitter.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        while True:\n",
    "            conn_r = connect_to_db()\n",
    "            try:\n",
    "                with conn_r.cursor() as cursor:\n",
    "                    docs = get_unprocessed_docs_batch(cursor, table_name, last_processed_id, batch_size)\n",
    "            finally:\n",
    "                conn_r.close()\n",
    "            \n",
    "            if not docs:\n",
    "                break\n",
    "            \n",
    "            # Break docs into smaller batches for parallel processing.\n",
    "            doc_batches = [docs[i:i + max(1, batch_size // max_workers)] \n",
    "                           for i in range(0, len(docs), max(1, batch_size // max_workers))]\n",
    "            \n",
    "            futures = [executor.submit(process_document_batch, batch, text_splitter, embed_model) \n",
    "                       for batch in doc_batches]\n",
    "            processed_docs = set()\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    chunks_data = future.result()\n",
    "                    doc_ids = {doc_id for doc_id, _, _, _ in chunks_data}\n",
    "                    insert_chunks(table_name, chunks_data)\n",
    "                    processed_docs.update(doc_ids)\n",
    "                    pbar.update(len(doc_ids))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing a batch: {e}\")\n",
    "            \n",
    "            if processed_docs:\n",
    "                current_max_id = max(processed_docs)\n",
    "                last_processed_id = max(last_processed_id, current_max_id)\n",
    "                update_checkpoint(checkpoint_table, last_processed_id)\n",
    "    pbar.close()\n",
    "    print(f\"All documents processed and stored in {table_name}.\")\n",
    "    \n",
    "    conn_ddl = connect_to_db()\n",
    "    try:\n",
    "        with conn_ddl.cursor() as cursor:\n",
    "            create_indexes(cursor, table_name)\n",
    "        conn_ddl.commit()\n",
    "    except Exception as e:\n",
    "        print(\"Error creating indexes:\", e)\n",
    "        conn_ddl.rollback()\n",
    "    finally:\n",
    "        conn_ddl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580429b-902e-4f21-9848-c051a73e40b4",
   "metadata": {
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optional: Clear tables for a fresh start.\n",
    "def clear_tables(model_name='all-MiniLM-L6-v2', confirm=False):\n",
    "    \"\"\"Delete the chunks and checkpoint tables for a given embedding model.\"\"\"\n",
    "    if not confirm:\n",
    "        confirmation = input(f\"Are you sure you want to delete all tables for model '{model_name}'? (y/n): \")\n",
    "        if confirmation.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return False\n",
    "\n",
    "    table_name = f\"s2docs_chunks_{model_name.replace('-', '_').replace('/', '_')}\"\n",
    "    checkpoint_table = f\"{table_name}_checkpoint\"\n",
    "    \n",
    "    conn = connect_to_db()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Drop vector index if it exists.\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM information_schema.statistics \n",
    "                WHERE table_schema = DATABASE() \n",
    "                  AND table_name = '{table_name}' \n",
    "                  AND index_name = 'idx_{table_name}_vector'\n",
    "            \"\"\")\n",
    "            if cursor.fetchone()[0] > 0:\n",
    "                try:\n",
    "                    cursor.execute(f\"DROP INDEX idx_{table_name}_vector ON {table_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning when dropping vector index: {e}\")\n",
    "            # Drop fulltext index if it exists.\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM information_schema.statistics \n",
    "                WHERE table_schema = DATABASE() \n",
    "                  AND table_name = '{table_name}' \n",
    "                  AND index_name = 'idx_{table_name}_text'\n",
    "            \"\"\")\n",
    "            if cursor.fetchone()[0] > 0:\n",
    "                try:\n",
    "                    cursor.execute(f\"DROP INDEX idx_{table_name}_text ON {table_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning when dropping FULLTEXT index: {e}\")\n",
    "            # Drop tables.\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {checkpoint_table}\")\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        conn.commit()\n",
    "        print(f\"Successfully deleted tables for model '{model_name}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting tables: {e}\")\n",
    "        conn.rollback()\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# clear_tables(model_name='all-MiniLM-L6-v2', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bf1a6-d24d-4330-b7b2-d89588bace41",
   "metadata": {
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Try multi processing \n",
    "# multiprocessing.set_start_method('spawn', force=True)\n",
    "# # Launch process_documents in a separate process.\n",
    "# p = multiprocessing.Process(target=process_documents, kwargs={'batch_size': 100, 'max_workers': 16})\n",
    "# p.start()\n",
    "# # Wait for up to 1200 seconds (20 minutes).\n",
    "# p.join(timeout=1200)\n",
    "\n",
    "# # If still running after timeout, terminate.\n",
    "# if p.is_alive():\n",
    "#     print(\"process_documents is still running after 20 minutes. Terminating...\")\n",
    "#     p.terminate()\n",
    "#     p.join()  # Wait for the process to terminate.\n",
    "#     print(\"process_documents terminated after 20 minutes.\")\n",
    "\n",
    "\n",
    "\n",
    "# Process documents with GPU acceleration. Default values for embedding models, chunk size, etc can be added.\n",
    "process_documents(batch_size=200, max_workers=16)"
   ]
  }
 ],
 "metadata": {
  "jupyterlab": {
   "notebooks": {
    "version_major": 6,
    "version_minor": 4
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "singlestore_cell_default_language": "python",
  "singlestore_connection": {
   "connectionID": "",
   "defaultDatabase": ""
  },
  "singlestore_row_limit": 300
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
